{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mining Data from the Web with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Purpose\n",
    "Summarize different strategies for mining data from the web and discuss when to use them so that you can become a more powerful and efficient data analyst.\n",
    "\n",
    "## Agenda\n",
    "- Data Collection\n",
    "- Data Available on the Web\n",
    "- Downloading Datasets\n",
    "- Overview of Web APIs\n",
    "- Python API Wrapper Libraries\n",
    "- Accesing Web APIs from Python\n",
    "- Web Scraping\n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection\n",
    "<img style=\"float: right;\" src=\"images/data_analysis_process.png\">\n",
    "When performing data analysis, analysts will typically follow a process similar to the one on the right.\n",
    "\n",
    "The focus of this talk is on the data collection step of the data analysis process.\n",
    "\n",
    "Data collection is \"gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes.\" [4]\n",
    "\n",
    "Data collection is important:\n",
    "- Significant impact to remaining steps in the data analysis process\n",
    "- Time intensive; as the old adage goes \"data scientists spend 80% of their time finding, cleaning and reorganizing data and only 20% conducting analysis.\" [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection Considerations\n",
    "There are many things to consider when determining how data will be collected for analysis.  Not only are there many techniques by which to collect data from a source there may also be many sources of that data.\n",
    "\n",
    "Below are the aspects to consider prior to collecting data [5]:\n",
    "- Accuraccy: Data providing the most accurate results is desired.\n",
    "- Reliability: Data that can be collected consistently is desired.\n",
    "- Time/Cost: Minimal implementation time and cost behind collecting the data is desired.\n",
    "- Utility: Data that can be easily analyzed is desired.  Data that contributes in a significant way to answering the question is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mining Data from the Web\n",
    "One of the most common sources of data is the internet. The web is often the only source of suitable data for analysis and frequently the most convenient.  More and more data exists on the web every day and more and more of it is easily accessible.\n",
    "\n",
    "The focus of the remainder of this talk is collecting data that we want to utilize for analysis.  A selection of techniques are discussed to help you better understand what your options are for implementing the data collection step of your analysis process.  The techniques are discussed in order of ease of implementation.  For example Downloading a File Locally is easier than Web Crawling.\n",
    "\n",
    "Web Data Extraction Techniques:\n",
    "- Downloading Datasets\n",
    "- Python API Wrapper Libraries\n",
    "- Accessing Web APIs from Python\n",
    "- Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Downloading Datasets\n",
    "Often times an analyst's first experience with collecting data from the web will be downloading a csv file locally and then using it from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x   y\n",
       "0 -3   9\n",
       "1 -2   4\n",
       "2 -1   1\n",
       "3  0   0\n",
       "4  1   1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample_data = pd.read_csv('sample.csv')\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is about as easy as it gets, and preferred to some of the more time intensive techniques covered later.  This is not specific to csv files.  Other file formats you might find as downloadable assets are json files (.json), HDF5 files (.h5), and excel files (.xls, .xlsx) to name a few.  The pandas library has utilities for reading those file types as well.\n",
    "\n",
    "The issue with this technique is that it is only applicable when someone else has done the leg work to consolidate the data and possibly clean it.  It is unlikely you will be able to collect all of your data in this manner if you are performing novel analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Web APIs\n",
    "Prior to discussing the remaining techniques it will be useful to introduce and give a brief overview of Web APIs.\n",
    "\n",
    "According to wikipedia a Web API \"is a programmatic interface consisting of one or more publicly exposed endpoints to a defined request–response message system, typically expressed in JSON or XML, which is exposed via the web—most commonly by means of an HTTP-based web server.\" [6]\n",
    "\n",
    "Web APIs, specifically how they apply to data collection, are probably best understood using an example of a single endpoint (A web API is one or more endpoints).  Lets use Github's API as an example.  Github's endpoint url is https://api.github.com.  From the documentation [7] I know that I can send a GET HTTP request to /users/haleysam93/repos to get some summary information about my github repos in JSON format.  \n",
    "\n",
    "Shown below is the recieved JSON when navigating to the URL https://api.github.com/users/haleysam93/repos using my web browser, which sends the GET HTTP request.\n",
    "\n",
    "<img src=\"images/github_api_response.png\">\n",
    "\n",
    "In the 'Using Web APIs' sections examples of how to send requests using python will be shown.  The purpose of this section is to introduce the concept of getting data from Web APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python API Wrapper Libraries\n",
    "Even though it is possible, as will soon be showed, to send requests and receive JSON using Python, it isn't necessarily pleasant.  It will at minimum require parsing the JSON object in order to pull the data that you care about and possibly much more tedious programming.\n",
    "\n",
    "To alleviate some of this burden many organizations will release libraries that provide a higher level interface to the data.  These libraries are typically called API wrappers.  Listed below is small set of common API wrappers I find useful for data collection.  For a more complete list checkout [Real Python's List of Python API Wrappers](https://github.com/realpython/list-of-python-api-wrappers).\n",
    "\n",
    "Common Python API Wrapper Libraries:\n",
    "- python-twitter: wrapper library for accessing tweets\n",
    "- newsapi-python: wrapper library for Google News\n",
    "- praw (Python Reddit API Wrapper): wrapper library for reddit data\n",
    "- google-maps-services-python: wrapper library for accessing various google maps services\n",
    "- nba_api: wrapper library for accessing nba statistics\n",
    "- rottentomatoes: wrapper library for accessing rotten tomatoes data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Mining tweets using the python-twitter library\n",
    "Twitter is a great source of text data.  Sentiment analysis, which is the process of using a model to determine the sentiment (positive, negative, neutral) of some text, is commonly performed using tweet data.  This example shows how one could pull all the tweets on Donald Trump's timeline using the twitter-python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/4FMs202NrW',\n",
       " 'Robert Mueller is being asked to testify yet again. He said he could only stick to the Report, &amp; that is what he wo… https://t.co/mAQc2kmO3t',\n",
       " 'Big 4th of July in D.C. “Salute to America.” The Pentagon &amp; our great Military Leaders are thrilled to be doing thi… https://t.co/UEPNdG57A1',\n",
       " 'The Economy is the BEST IT HAS EVER BEEN! Even much of the Fake News is giving me credit for that!',\n",
       " 'As most people are aware, according to the Polls, I won EVERY debate, including the three with Crooked Hillary Clin… https://t.co/g5pKI6Px2X',\n",
       " 'Mark Levin has written a big number one bestselling book called, conspicuously and accurately, “Unfreedom of the Pr… https://t.co/fBdKrF5bFp',\n",
       " '...Texas will defend them &amp; indemnify them against political harassment by New York State and Governor Cuomo. So ma… https://t.co/0UZzWARrml',\n",
       " 'People are fleeing New York like never before. If they own a business, they are twice as likely to flee. And if the… https://t.co/g8CC4el3kT',\n",
       " 'I will be interviewed by @TuckerCarlson tonight at 8:00pm on @FoxNews!',\n",
       " '....In the meantime, our teams will be meeting to work on some solutions to very long term and persistent problems.… https://t.co/mbaTTcTaxt',\n",
       " 'It was great being with Chairman Kim Jong Un of North Korea this weekend. We had a great meeting, he looked really… https://t.co/wImoq9TtBZ',\n",
       " 'I am excited to announce that @MercedesSchlapp will soon be joining our Campaign. She feels so strongly about our C… https://t.co/V7M75k694t',\n",
       " 'That’s right, The Trump Foundation gave away 100% plus, with Zero rent or expenses charged, and has been being sued… https://t.co/MBdkc5rQZb',\n",
       " '....New York businesses in search of anything at all they can find to make me look as bad as possible. So, on top o… https://t.co/fFb8Ry0miS',\n",
       " '....more money than it had. Going on for years, originally brought by Crooked Hillary’s Campaign Chair, A.G. Eric S… https://t.co/UvqztWIsXx',\n",
       " 'It is very hard and expensive to live in New York. Governor Andrew Cuomo uses his Attorney General as a bludgeoning… https://t.co/jPbW9Iicxj',\n",
       " '.....why President Trump has to deal with North Korea the way it is now. He had to figure out what to do with the K… https://t.co/XaX9WuFkpj',\n",
       " '“In my opinion the President has done more good on the Korean issue in the last year and a half than President Obam… https://t.co/hWVYi5LQK6',\n",
       " 'https://t.co/k24a6VMTFu',\n",
       " 'Congratulations to Prime Minister Abe of Japan for hosting such a fantastic and well run G-20. There wasn’t a thing… https://t.co/BB5u9hCUA2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key='NKjwPloSr1V5tejcE8LTr5uzL',\n",
    "                  consumer_secret='tM57R1zNbZG8nyK4ztEDpbeosOw5qBRUOdS18oHGiMddkZpE0v',\n",
    "                  access_token_key='1705497168-Q3xGfgcaHO9tiHCy8m8hkUQbFd8jGOjS0kK46DP',\n",
    "                  access_token_secret='cqcO5kVjaNGy8IXpLIBdp7wEGsfsn6w9hEHL2tBFEQZNy')\n",
    "tweets = [x.text for x in api.GetUserTimeline(screen_name='realDonaldTrump')]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The above code didn't necessarily provide us with a capability we wouldn't otherwise have it did make it a lot easier.  Wrapper libraries can use the full power of the python library and provide sensible python objects as opposed to HTTP requests which return text data that requires parsing.\n",
    "\n",
    "Similarly to simply reading a file, however this technique also suffers from only being supported by companies with the resources to build a python library to access their data.  If you are accessing data from a common source such as twitter, facebook, google or amazon there will likely be a library to support you.  For novel analysis it is unlikely you will be able to access all of your data in this fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accessing Web APIs with Python\n",
    "We have already discussed what a Web API is, but we did not show how to access the data they provide from Python.  This section will cover how Web APIs can be used from within Python.\n",
    "\n",
    "In order to send requests and recieve their responses we will be using the requests library.  I like requests for it's very simple user interface.  The barrier to entry is low allowing pretty much anyone to send their first requests in minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Sending Requests to the Github API  \n",
    "For this example we will bring back the github API and show how to access it programatically using the requests library.  Our goal will be to get information on all the repositories that a user (me in this example) has on github.  We will then store the returned data in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>last_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haleysam93/beer_description_generator</td>\n",
       "      <td>Simple Web App Generating Fake Beer Descriptions</td>\n",
       "      <td>https://api.github.com/repos/haleysam93/beer_d...</td>\n",
       "      <td>2019-05-09T19:48:52Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haleysam93/bokeh</td>\n",
       "      <td>Interactive Web Plotting for Python</td>\n",
       "      <td>https://api.github.com/repos/haleysam93/bokeh</td>\n",
       "      <td>2018-07-29T21:19:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>haleysam93/nfl_dashboard</td>\n",
       "      <td>Dashboard displaying nfl data</td>\n",
       "      <td>https://api.github.com/repos/haleysam93/nfl_da...</td>\n",
       "      <td>2018-12-15T02:05:10Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haleysam93/PythonDataScienceHandbook</td>\n",
       "      <td>Python Data Science Handbook: full text in Jup...</td>\n",
       "      <td>https://api.github.com/repos/haleysam93/Python...</td>\n",
       "      <td>2018-07-19T02:21:23Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haleysam93/web_data_mining_talk</td>\n",
       "      <td>Some conference talk ideas targeted primarily ...</td>\n",
       "      <td>https://api.github.com/repos/haleysam93/web_da...</td>\n",
       "      <td>2019-07-02T19:49:23Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               repo_name  \\\n",
       "0  haleysam93/beer_description_generator   \n",
       "1                       haleysam93/bokeh   \n",
       "2               haleysam93/nfl_dashboard   \n",
       "3   haleysam93/PythonDataScienceHandbook   \n",
       "4        haleysam93/web_data_mining_talk   \n",
       "\n",
       "                                         description  \\\n",
       "0   Simple Web App Generating Fake Beer Descriptions   \n",
       "1                Interactive Web Plotting for Python   \n",
       "2                     Dashboard displaying nfl data    \n",
       "3  Python Data Science Handbook: full text in Jup...   \n",
       "4  Some conference talk ideas targeted primarily ...   \n",
       "\n",
       "                                                 url          last_updated  \n",
       "0  https://api.github.com/repos/haleysam93/beer_d...  2019-05-09T19:48:52Z  \n",
       "1      https://api.github.com/repos/haleysam93/bokeh  2018-07-29T21:19:13Z  \n",
       "2  https://api.github.com/repos/haleysam93/nfl_da...  2018-12-15T02:05:10Z  \n",
       "3  https://api.github.com/repos/haleysam93/Python...  2018-07-19T02:21:23Z  \n",
       "4  https://api.github.com/repos/haleysam93/web_da...  2019-07-02T19:49:23Z  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests as r\n",
    "\n",
    "resp = r.get(\"https://api.github.com/users/haleysam93/repos\")\n",
    "\n",
    "all_repos = []\n",
    "for repo in resp.json():\n",
    "    all_repos.append((repo['full_name'], repo['description'],\n",
    "                      repo['url'], repo['updated_at']))\n",
    "\n",
    "all_repos = pd.DataFrame(all_repos, columns=['repo_name', 'description', 'url', 'last_updated'])\n",
    "all_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That was quite simple.  We actually made the request using only a single line of code, which returned us a Response object.  The json method of the Response object provided us a list of dictionaries representing the json body of the HTTP response.  All that was left to do was pull out the desired information from each dictionary and consolidate it into a DataFrame.\n",
    "\n",
    "Often times when attempting to collect data you will be making multiple requests.  Many APIs will limit the rate at which you can make requests.  The Github API for example limits users to 60 requests per hour for unauthenticated requests and 5000 requests per hour for authenticated requests.  If you exceed those limits your receive error responses.  I recommend taking a look at each APIs documentation prior to using it to understand what the best practices are.  It can save a lot of headache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Private Undocumented APIs\n",
    "In the last section we showed an example using the Github API, which is a public documented API.  Github provides that API and publishes documentation on how to use it as an open invitation for anyone to use it.  There are many organizations, however, that don't provide a public API, but still rely on private APIs for accessing data within their applications.\n",
    "\n",
    "This is where we get to put on our hacker hats and do a little reverse engineering.  The basic idea is to use the developer tools of our web browser (firefox is used in my examples) and look at the requests that are being sent by the web application we are using.  If we can determine the structure of the requests being sent we can mirror them using the requests library and get the same data that the web application is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Mining ESPN Fantasy Football Data\n",
    "During the most recent Fantasy Football season I wanted to get an edge on my competition.  I figured the best way to do this was to use some of my data analysis skills.  I figured that if I could acquire the data I would be able to identify my team's weaknesses and then set out to improve upon them.\n",
    "\n",
    "When I got to my all important data collection step I realized there was an issue.  There was no way for me to download my league's data and ESPN Fantasy Football does not have a public documented API.  I did some research and found some examples of people accessing data using ESPN fantasy football's private API.  They were not pulling the same data that I was interested in, but I at least had a starting point to work with.\n",
    "\n",
    "#### Step One: Determining the Right Endpoint\n",
    "The data that I was interested in was the number of points that each roster spot scored each week.  I figured that page showing scores would get that information.  In order to determine how the ESPN web app was sending those requests I used firefox's developer tools to look at the requests that were being sent.\n",
    "\n",
    "The general steps for doing this within firefox are:\n",
    "- Open developer tools (CTRL+SHIFT+I)\n",
    "- Click on the 'Network' tab\n",
    "- Click the tab to filter to just XHR\n",
    "\n",
    "<img src=\"images/espn_ff_private_api_capture.png\">\n",
    "\n",
    "Once I had firefox displaying the requests that were being sent I simply navigated to the page that accessed the information I was interested in and watched while the list of requests was filled in.  There were other requests for things like ads, but using some common sense I was able to isolate the request that was most likely for the data.  I know had a URL and set of parameters to work with.\n",
    "\n",
    "#### Step Two: Handling Authentification\n",
    "Being slightly over zealous I immediately tried to replicate the request using Python.  I subsequently received a 403 error letting me know that my request was forbidden.  I went back and did more research and realized that because my league was private I needed to authenticate my request.  The easiest way to do this was to send cookies along with my request that would authenticate it.\n",
    "\n",
    "Once again my trusty web browser saved the day.  Using the web browser's developer tools I was able to determine the correct values for those cookies.  Requests made it really easy to include those in my request as well.\n",
    "\n",
    "#### Step Three: Putting it All Together\n",
    "Once I determined how to get a single request working the rest of the work involved writing the code to iterate through each week and subsequently each matchup within each week to compile the information I was interested in into a pandas DataFrame.  I then had the data I need to perform analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Web Scraping\n",
    "The final technique we will cover is web scraping.  Prior to diving in it is useful to define what web scraping is and is not for the purposes of this talk.  Web scraping is the process of parsing html documents to extract the information you want.  The 'web scraping' term often gets used interchangably with 'web crawling', but I want to make a clear distinction.  Web crawling typically involves using \"bots\" or \"spiders\" to systematically traverse entire websites.  The focus here will be on web scraping, and although Python does have a nice web crawling library in Scrapy covering web crawling is outside the scope of this talk.\n",
    "\n",
    "It is important to understand when you need web scraping and when you do not.  Web scraping is discussed last for a reason, because it can be time intensive and prone to issues.  I see it as a last resort for the case when the only way to access the data you want is by parsing values out of the HTML.\n",
    "\n",
    "The web scraping library that we will cover in this section is BeautifulSoup.  We will also need the requests library to make the HTTP requests to get the HTML text for BeautifulSoup to parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Mining Beer Description Data\n",
    "I recently worked on a project where the goal was to build a text model that would generate fake beer descriptions.  In order to build this model I needed real beer descriptions that I could train my model on.  Unfortunately this data is not publicly available, and after performing lots of research the only place I could find the descriptions that suited what I was trying to generate was on beeradvocate.com.  Unfortunately Beer Advocate is not very forth coming with there data and they do not provide any data sets or have an API.  The only way I could get the beer description was to go to the profile of each beer.\n",
    "\n",
    "#### Step One: Get the URLs to Request\n",
    "I was able to determine that the beer profile pages don't contain the name of the beer within the URL and instead the URL simply contains an integer between 1 and 400000 (seemingly an index).  This made life easier because I didn't have to know the name or producer of the beer, I could simply iterate from 1 to 400000 appending the value to the base URL.  The code to generate the list of URLs for every beer is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://www.beeradvocate.com/beer/profile/001/'\n",
    "beer_idxs = range(1, 400000)\n",
    "urls = [BASE_URL + str(i) for i in beer_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Step Two: Create the Function to Parse the HTML\n",
    "Every request that gets made will return a requests Response object so it's best to make the request and immediately parse the returned HTML so Python doesn't have to persist all of that data.  I created a function to perform the parsing which is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_description(html_text):\n",
    "    \"\"\"Parse the HTML text provided to extract the beer description text.\n",
    "    Return None if the beer description is not found or N/A.\"\"\"\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    info_box = soup.find('div', id='info_box')\n",
    "    if not info_box:\n",
    "        return None\n",
    "    desc_header = info_box.find('b', string=\"Notes / Commercial Description:\")\n",
    "    if not desc_header:    \n",
    "        return None\n",
    "    i = 0\n",
    "    while i < 3:\n",
    "        elem = desc_header.find_next(string=re.compile(\"\\n\\S\"))\n",
    "        if hasattr(elem, 'string') and len(elem.string) > 0:\n",
    "            text = elem.string.strip('\\n')\n",
    "            if not text.startswith(\"None\"):\n",
    "                return text\n",
    "            return None\n",
    "        i += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This function is non trivial because of the edge cases that it needs to handle.  If the HTML tag we are looking for to identify where the data is (info_box) doesn't exist or the Commercial Description header does not exist the function must handle that.  When I was implementing this functionality I essentially had to go through a trial and error phase, where I would solve the issue as I came accross them.  This is why web scraping can be time intensive.\n",
    "\n",
    "#### Step Three: Putting it All Together\n",
    "I now have my list of URLs and my function to parse the beer description from the HTML.  Now I need to implement the code to iterate over the URLs, get the HTML for each one and then parse that HTML.  Previously I mentioned that we don't want python to persist the requests Response objects.  We get around this by making the requests within a generator.  The python kernel will not have to persist this object in memory once we are done with it.  The code for doing this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Amber is a Munich style lager brewed with crystal malt and Perle hops. It has a smooth, malty, slightly caramel flavor and a rich amber color. Abita Amber was the first beer offered by the brewery and continues to be our leading seller.',\n",
       " 'Turbodog is a dark brown ale brewed with Willamette hops and a combination of pale, crystal and chocolate malts. This combination gives Turbodog its rich body and color and a sweet chocolate toffee-like flavor. Turbodog began as a specialty ale but has gained a huge loyal following and has become one of our flagship brews.',\n",
       " 'Experience the magic of Purple Haze.® Clouds of real raspberries swirl in this tart and tantalizing lager inspired by the good spirits and dark mysteries of New Orleans. Brewed with pilsner and wheat malts along with Vanguard hops, let the scent of berries in the hazy purple brew put a spell on you.',\n",
       " 'Wheat (May – September) German brewers discovered centuries ago that the addition of wheat produces a distinctively light, refreshing beer. Unlike traditional German wheat beers produced by other breweries, Abita Wheat is a lager, not an ale, and contains a generous amount of wheat which produces a clean, simple flavor.',\n",
       " 'Golden is a crisp, clean continental lager. Just four ingredients are all it takes: American malt, Mt. Hood hops, German lager yeast and pure Abita Springs water. As the name implies, Abita Golden has a brilliant gold color. Its flavor makes this light lager the perfect match for Louisiana Creole food.',\n",
       " 'Allagash Dubbel boasts a deep red color and a complex malty taste. The finish is dry with subtle hints of chocolate and nuts. The yeast asserts itself by lending a classic Belgian fruitiness.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as r\n",
    "\n",
    "# Go through only the first 10 urls for demo purposes\n",
    "resp_generator = (r.get(url) for url in urls[0:10])\n",
    "\n",
    "beer_descs = [parse_description(resp.text) for resp in resp_generator]\n",
    "beer_descs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While this was a relatively simple example hopefully demonstrates what Web Scraping is and how to use the BeautifulSoup to accomplish it with Python.  Hopefully it also drives the point that it is best to avoid web scraping if possible.  When going the web scraping route the complexity of the code increases quickly.  It is much easier to get data from files and/or using a library or API.  The time you spend collecting your data takes time away from actually exploring it and analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "In this talk we discussed 4 methodologies for mining data from the web in implementation complexity order:\n",
    "- Downloading Datasets\n",
    "- Python API Wrapper Libraries\n",
    "- Accessing Web APIs from Python\n",
    "- Web Scraping\n",
    "\n",
    "Hopefully the next time you need to collect data for a data analysis process you can utilize these methodologies to make your data collection step as painless as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sources\n",
    "[1] https://www.tutorialspoint.com/excel_data_analysis/data_analysis_process.htm\n",
    "\n",
    "[2] https://ori.hhs.gov/education/products/n_illinois_u/datamanagement/dctopic.html\n",
    "\n",
    "[3] https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Data_collection\n",
    "\n",
    "[5] https://ebrary.net/1291/education/considerations_for_collecting_data\n",
    "\n",
    "[6] https://en.wikipedia.org/wiki/Web_API\n",
    "\n",
    "[7] https://developer.github.com/v3/\n",
    "\n",
    "[8] https://syntax.fm/show/060/the-undocumented-web-scraping-private-apis-proxies-and-alternative-solutions\n",
    "\n",
    "[9] https://stmorse.github.io/journal/espn-fantasy-3-python.html\n",
    "\n",
    "[10] https://www.promptcloud.com/blog/data-scraping-vs-data-crawling"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
